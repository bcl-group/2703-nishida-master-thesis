# 論文紹介"Reinforcement learning control of a biomechanical model of the upper extremity"

## 文献情報

### 著者: Florian Fischer, Miroslav Bachinski, Markus Klar, Arthur Fleig & Jörg Müller

### 雑誌名: Scientific Reports

### 巻・号: 11.1

### 出版年: 2021

## Keyword
- フィッツの法則
  - $$MT = a + blog_2 (\frac{D}{W} + 1) $$
  - D:ターゲットとの距離
  - W：ターゲットの幅
  - MT：移動時間
- $$\frac{2}{3}Power Law$$  
  -  $$v_n = kP_n^{1-\beta} $$
  -  より高い曲率がより低い速度につながる
  -  $$v_n:接線速度$$
  -  $$p_n:曲率半径(その瞬間の軌道を最もよく近似する円の半径)$$
-  degrees of freedom (DOFs):自由度

## イントロダクションß
- 単純な上肢のモデル（出力が関節トルクや２次元平面）の場合、フィッツの法則と2/3累乗法則の両方が、信号に依存して定常的に発生するモーターノイズの下で、移動時間を最小化するという結果を得るために見られる
- 運動制御の初期の多くの研究は、生体力学を線形ダイナミクスまたはリンクセグメントモデルを備えた点質量モデルとしてモデル化しているが、リアリティと忠実度を高める生体力学モデルへの関心が高まっている
- 最近の研究：筋肉ベースのモデルの制御を可能とするために完全な生体力学モデルの単純化を行う
  - 自由度を縮小
  - 間接を固定して２次元
  - 独立して活動する筋肉の量を大幅に減らす
- これまで、これらの上記のモデルでは、生成された動きのリアリズム、特にフィッツの法則や2/3パワーの法則などの人間の動きの特徴を示すかどうかについて評価されていない
- 3次元においての上肢の完璧な骨格モデルでもフィッツの法則と2/3累乗法則の両方が、信号に依存して定常的に発生するモーターノイズの下で、移動時間を最小化するという直接的な結果を構成するために見られるということを確認することが目的

### 研究手法
- タスク内容
  - 学習フェーズ  
    - １エピソードが１５０step
    - 各ステップが0.01 s
    - 終了条件
      - 手先とターゲット球の距離　>= ターゲット球の半径の状態を0.1s継続
      - １５０step経過
    - ターゲットの位置
      - 高さ70cm × 幅40cm × 奥行30cm の直方体内から一様分布に基づいてサンプリング
      - 中心位置は身体の前方50cm、右肩の右側10cm
    - 初期姿勢
      - 初期関節角度：立方体の頂点に沿って配置された１２個のターゲットによって作られた凸包の中から一様分布に基づいて、サンプリング
      - 初期関節角度：[−0.005 rad/s, 0.005 rad/s]の範囲で一様分布に基づきサンプリング
    - 基本ループの流れ
      - 状態に対して、方策に基づき行動をサンプリング
      - それに基づき報酬を得て、状態と体の姿勢を更新
      - 学習する前に最初の1万stepの軌道をバッファーに保存
      - 毎回サンプリングフェーズにつき、1 stepの軌道を保存
      - １万stepごとに学習フェーズから評価フェーズに切り替わり、30episodeの間、policyをgreedyに変更して評価を行う
      - 評価によって、球の半径が変更（１cmを下回るまで更新）

  - フィッツの法則タスク
    - 前方５０cm、右肩の方向１０cmの地点を中心とした円状に1２個のターゲットを設置 
    - 図２.aに示すようにD、Wの１０個の組み合わせで各方向に５０回リーチングし、合計で6500回実行
    - ターゲット球は1.5s経過、もしくは手先とターゲット球の距離　>= ターゲット球の半径の状態を0.1s継続で変化
    
  - 楕円タスク
    - 特定の経路を正確にたどるようなシミュレーション
    - 初期位置は楕円の左端
    - 最初のターゲットは楕円の10%分の長さを時計回りに進んだ位置に設定
    - 楕円の上をなぞる軌道で、10%の半分以上進んだら、次のターゲットが提示（少し前倒しで動き続ける）
    - 評価として、水平方向と垂直方向の直径が15cmと6cmの楕円を使用し、その中心は前方55cm、肩の高さより10cm、右肩方向に10cmの位置に配置
    - タスクは1分間実行、エンドエフェクターの位置、速度、加速度は10 msごとに保存。
  
- 制御対象
  - 7 つのphysical bodies と 5 つの”phantom” bodiesで構成
  - ３つの関節に対して、７つの自由度が対応し、５つの更なる関節に対して自由度によって拘束された13の要素が対応している
  - 強化学習モデルを実装するために高速mujocoシミュレーションで上肢のダイナミックモデルを実装
  - 個々の筋肉を使うと（離散的な行動の場合）アクション空間が指数関数的に増えてしまうから各自由度に対して集約された筋肉のアクションを表すアクチュエータを実装
  - 式(4)の説明
    - 集約された筋肉のアクションを表すアクチュエータ
    - $$c_n^{(q)}:各自由度 q において適用した制御（調整されるパラメータ）$$
    - $$\sigma_n^{(q)}:各自由度 q の活性の結果$$
    - $$t_e,t_a:固定励起と活性化の時間定数$$
    - より正確な値を出すためにそれぞれのステップごとに 5 サブステップ（2 ms）を計算（Δtのみ変更）
  - 式（５）の説明
    - $$a_n^{(q)}:ポリシーによって学習されたアクションベクトル$$
    - $$\eta_n^{(q)},\epsilon_n:平均０、標準偏差 0.103、0.185のガウス分布に従うランダムな値$$
  - 式（６）の説明
    - 各自由度に対応するトルク
    - $$g^{(q)}:筋肉群の強さを表すスケーリング係数$$
    - $$\eta_n^{(q)},\epsilon_n:平均０、標準偏差 0.103、0.185のガウス分布に従うランダムな値$$
  - トルクの制限、関節角度の制限は表1に記載
- 強化学習
  - ポリシーはSAC
  - 式（８）の説明
    - 状態に基づく全ての行動に対する方策の確率分布のエントロピー
  - 式（９）の説明
    - 目的関数
    - $$\gamma:割引率$$
    - $$\alpha:自動調節できるようにθともに学習させるパラメータ。\alphaが大きいとランダム性が強い行動となり、低いとより確実的な行動を選ぶようになる。それを報酬によって決めるのは難しい$$
  - 図６の説明
    - ニューラルネットワークのアーキテクチャ
    - 活性化関数はrel、全結合
    - double Qlearning
      - 同じ構造のcriticnetworkをもう一つ準備
      - 学習の速度と安定性の向上
      - Q学習ではQ値が過大評価されるという問題がある
      - 両者の最低値を出力されるQ値とする
    - 状態
      - 各自由度に対する関節角度（７つ）
      - 各自由度に対する関節角速度（７つ）
      - $$\sigma_n^{(q)}、\dot{\sigma}_n^{(q)}:各自由度 q の活性の結果と興奮の結果$$
      - 手先とターゲット球の位置（３次元×２）
      - 手先とターゲット球の速度（３次元×２）（ターゲット球の速度はほぼ０）
      - 手先の加速度（３次元）
      - 手先からターゲット球への差分ベクトル（３次元）
      - 手先の速度がターゲット方向にどれだけ向いているか（ターゲットまでの単位方向ベクトルと速度ベクトルの内積）（１つ）
      - ターゲット球の半径（１つ）
      - ターゲット球の速度、手先の加速度、差分ベクトル、および手先の速度の投影は、なくても困らないが、あっても学習時間には関係なく、より複雑なタスクに役立つ可能性があるため、組み込んだ。
  - 関節にかかわる力として、式（６）のトルクだけではなく、能動的な追加のトルク（他の関節からのトルク）だったり、受動的な力（重力など）が働く
  - 各自由度の最大トルクは実際に実験を行なって得た値に基づいて決めた（表１）
  - 報酬
    - Harris & Wolpertの主張1:中枢神経系（CNS）がトルクの変化量とか、加速度・ジャーク（躍度）をわざわざ計算して最小化してるという根拠はなく、その値を保存したり統合したりすることができるかどうかさえ明らかではない
    - Harris & Wolpertの主張2:移動時間を固定した場合、信号に依存するノイズの中で終点の手先の分散を最小にしようとしている
    - 許容される手先の終点分散がターゲットの大きさで決まっている時、動作時間を最小にしようとすることに相当する
    - 式（１０）の説明
      - 全てのタイムステップに対して均等に罰する
      - 時間に対する罰しか与えない
- 学習方法
  - episodeが終了して初めて報酬を得られるという設定では良い行動と悪い行動の違いが明確にならずが学習が困難
  - それぞれの筋肉の活動量による正しい行動の組み合わせは非常に狭いため、ランダムな探索でうまくいく方策を見つけるのは難しい
  - 最初は完全ランダムな行動をサンプリング＝十分な探索を行う
  - ターゲット球の半径を大きくすることで到達させる
  - トレーニング中にターゲット球の幅を適切に収縮させるのは難しく、ほとんどの場合タスクの難易度が急上昇するため、全く到達しなくなったり、変な挙動を起こすようになる
  - 成功率に基づいてターゲット球の大きさを変化
    - 最初は直径60cm
    - 10000エピソードのうち、最新の30回のエピソードに応じてターゲット球の大きさが変化
    - 成功率90%以上: -1 cm
    - 成功率70%未満: +1 cm
    - 直径の値の暴走はクリッピング処理で[-0.1,60]とし、防ぐ
    - 10%の確率で0.1cm~60cmの一様分布に基づき、一時的に球の直径がランダムに変化（成功率に含まれる）
    - 上記のランダム性は過去の簡単なタスクを解決する方法を忘れる壊滅的な忘却を防ぐ働き
### 結果
- fig２
  - 6500回全部成功
  - 横軸ID、縦軸時間
  - 中央値に対して、R値を出している
- fig3
  - 累乗則を直線として扱うために、両辺にlogとっている
  - 横軸曲率半径、縦軸手先の速度
  - R値が0.84
  - 60秒間手先を動かして100hzでサンプリング
- fig4、fig5
  - ID4,ID2（距離0.35）の時のフィッツの法則タスクの結果
  - 軌道が少し曲がっている
  - 滑らかに加速と減速を行う
  - IDが大きくなると目標地点に対して最初に急速な動きを示し、修正時間が長くなる
  - IDが減ると値のばらつきが出る
  - （補足）ID2で加速度がマイナスになるのは直前の動きで目標にピッタリ届かなかった場合、それを修正するために最初の加速度を調整
  
