# 論文紹介"Constraints as Rewards: Reinforcement Learning for Robots without Reward Functions."

## 文献情報

### 著者: Yu Ishihara, Noriaki Takasugi, Kotaro Kawakami, Masaya Kinoshita, Kazumi Aoyama

### 雑誌名:  

### 出版年: 2025

## Keyword


### 概要（導入）
- 強化学習の進歩によりロボットは複雑な操作や移動を生成できるようになったが、多くの成功はアルゴリズムの進歩だけでなく、報酬関数の調整（報酬工学）によるところが大きい。(例）4つの異なる目的からなる報酬を用いて、これらの目的間の重みを調整して、人間の動きを模倣する二足歩行ロボットの移動行動を達成)
- 報酬を複数の目的（安全性、安定性、姿勢保持、効率など）で設計すると、目的間の重み調整が必要になり、その調整は設計者の暗黙知に依存しやすい。
- 本研究は、報酬設計の試行錯誤を排し、制約（constraints）だけで課題を定式化する手法（Constraints as Rewards：CaR）を提案する
- CaR ではタスクを制約関数のみで定義し、報酬関数を排除する。ラグランジアン法により制約違反の重み（ラグランジュ乗数）を自動調整することで、従来の重みチューニングを置き換える。
- アルゴリズム実装として、QRSAC（既存手法）の拡張である QRSAC-Lagrangian を提案
- シミュレーション・実ロボット実験で、既存手法より学習が速く安定することを示す

## 今回の論文を理解するための準備
- 強化学習の目的は、運動時間 $T$ における割引報酬和を最大化する最適方策 $\pi^*$ を求めることである。
-   $\max_{\pi} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t r(s_t, a_t) \right].$
    - $s_t$, $a_t$, $\gamma$, $r$ はそれぞれ、時刻 $t$ における状態、行動、割引率、および報酬関数 
- 多くの実際の応用では、報酬関数 $r(s,a)$ は複数の要素関数 $r_n(s,a)\ (n=1,\dots,N)$ の重み付き和として設計  
- $\max_{\pi} \sum_{n=1}^{N} w_n \mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t r_n(s_t, a_t) \right].$
  - $r(s,a) = \sum_{n=1}^{N} w_n r_n(s,a).$
- 問題点：成功する学習のためには重み $w_n$ と関数 $r_n$ の両方を適切に調整する必要があるが、これらのパラメータを体系的に調整する方法は存在しない。
- この研究では、報酬関数設計における試行錯誤を軽減するために、制約付き強化学習問題の枠組みを用いる。  
- $\max_{\pi} \quad  \mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t r(s_t, a_t) \right]\text {s.t.} \quad  \mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t g_m(s_t, a_t) \right] \ge 0, \quad m = 1, \dots, M.$
  - $g_m(s_t, a_t)$ は $m$ 番目の制約関数  
- CaRでは、上式 で表される制約付き強化学習問題を、ラグランジュ双対関数 $L(\pi, \lambda)$ を導入することで非制約問題に変換。
- $\max_{\lambda_m > 0\ \forall m} \max_{\pi} L(\pi, \lambda).$
  - $\lambda = [\lambda_1, \dots, \lambda_M]^T$ はラグランジュ乗数
  - $L(\pi, \lambda)\triangleq\mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t r(s_t, a_t) \right]+\sum_{m=1}^{M} \lambda_m\mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t g_m(s_t, a_t) \right].$
- CaRの核心的なアイデアは、報酬関数を $r(s_t, a_t) \triangleq 0$ と設定することであり、次の問題が得られる。
- $\max_{\lambda_m > 0\ \forall m} \max_{\pi}\sum_{m=1}^{M} \lambda_m\mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t g_m(s_t, a_t) \right].$
  - 目的関数を制約条件の形で設計することで、複数の制約間の重み付けを表すラグランジュ乗数が自動的に調整されるため、異なる目的間の重みを手動で調整することなく、望ましい方策を得ることができる。

## ロボットと制約関数の説明
### ロボットのハードウェア概要と制約関数設計
- ロボットは6本の伸縮脚 (telescopic legs) を持ち、そのうち4本は駆動車輪付き（モータで回転させられる車輪）、2本は全方向パッシブホイール（モータで駆動されない自由回転の車輪）を備える
- 各脚にはヒップ関節（可動範囲 $45^\circ$）と，最大$500\,mm$ まで伸縮するプリズマティック（直動）膝関節を備える．
- 方策は各脚の協調を行い，タスク中にロボットが自律的に立ち上がれるようにする必要がある．

### 立ち上がりタスクのための制約関数設計
- 各制約は直感的に解釈可能な最適化目標を与える．
- 時刻依存の制約（timestep constraints）
  - 特定の時刻におけるロボットの状態や動作を制約する（例：最終姿勢の制約）ことを可能にする
- エピソード依存の制約（episode constraints）
  - エピソード全体を通しての制約（例：障害物と衝突しない）を可能にする．
- 直感的な解釈を与えることでタスク設計者にとって目的記述が容易になることが期待される
- 本実験ではこれらの関数を組み合わせてタスク目的を定義した．なお，各制約における不等号は，制約関数を $-g(s,a)$ とすることで逆向きにすることができる．各関数の導出は付録に記載する．

### 時刻値制約 (Timestep value constraint)
- $\epsilon \ge \mathbb{E}_{\pi}[\hat{g}(s_{t'}, a_{t'})].$
- 特定時刻 $t=t'$ における状態、行動から計算される値 $\hat{g}(s,a)$ の期待値 が $\epsilon$ 以下であることを制約する
- $g(s,a) =\begin{cases}0, & (t \neq t'),\\[4pt]\epsilon - \hat{g}(s,a), & (t = t').\end{cases}$
#### 最終姿勢（Final pose）制約
- 最終時刻 $t=T$ における関節角および膝位置（足がどれだけ伸びているか）が所定の閾値以下になるよう制約する．
- $g(s,a)=\begin{cases}0, & (t\neq T),\\[4pt]10^{-3} - \big| s_{\mathrm{target}} - s_{\mathrm{angle/position}} \big|, & (t = T).\end{cases}$
- 既存研究 では各時刻ごとに姿勢を罰するエピソード報酬を用いる例があるが，制約の観点からは中間時刻の姿勢は任意であるべき．
- 本研究では各脚に対してヒップ角と膝位置の最終値をそれぞれ閾値（$10^{-3}$ rad / $10^{-3}$ m）以内に収めるよう制約し，合計で12個の姿勢制約を定義した．
#### 傾斜（Inclination）制約
- 最終姿勢が地面に対して平行になるよう，ロボット本体が鉛直軸（z 軸）に対して直交することを制約する．
- $g(s,a)=\begin{cases}0, & (t\neq T),\\[4pt]10^{-2} - \big| \bold{u}_z^{(t)} \bold{v}_{x,y} \big|, & (t = T).\end{cases}$
- $\bold{u}_z=[0,0,1]^T$
- $\bold{v}_{x,y}$：足の関節をつなぐ正規化されたベクトル
- 上式の $u_z^{(T)} v_{x,y}$ は最終時刻における本体の鉛直成分と水平面成分の関係を表す指標であり，ロボットの姿勢が鉛直軸に対してどれだけ傾いているかを測る量である．本制約により最終姿勢が床と平行になることを促す．


### 時刻確率制約 (Timestep probability constraint) 
- $p_{\epsilon}\in[0,1] :[p_{\epsilon} \ge P_{\pi}(s_{t'}\in S', a_{t'}\in A')]$
- ある特定の時刻$t=t'$ における状態が$s_{t'}$であり、行動が $a_{t'}$である確率を$p_{\epsilon}$以下にあるように方策とラグランジュ乗数を調整するための制約
- $g(s,a) =\begin{cases}0, & (t \neq t'),\\[4pt]p_{\epsilon} - \mathbb{1}_{\{s\in S',\,a\in A'\}}, & (t = t'),\end{cases}$
- $\mathbb{1}$は指示関数
- $S'$ および $A'$ は状態と行動の集合
- $P_{\pi}$ は状態行動確率
####  倒立（Fall down）制約
- 転倒確率を0に制約する
- ロボットが倒れた状態とは「5本超のホイール関節がヒップ関節より高位置にある」場合と定義し，タスクは倒れた時点で終了するため，指示関数は最終時刻 $t=T$ で評価される
- $g(s,a)=\begin{cases}0, & (t\neq T),\\[4pt]-\,\mathbb{1}[\text{robot is in a fall-down state}], & (t = T).\end{cases}$

### エピソード確率制約 (Episode probability constraint) 
- $p_{\epsilon}\in[0,1]:p_{\epsilon} \ge P_{\pi,\gamma}(s\in S', a\in A')$
- エピソード中にその事象が生じる確率が $p_{\epsilon}\in[0,1]$ 以下であることを制約する
- $g(s,a) = p_{\epsilon} - \mathbb{1}_{\{s\in S',\,a\in A'\}}.$
- $P_{\pi,\gamma}$ は割引された状態行動確率
#### 本体接触（Body contact）制約
- エピソード中にロボット本体が床に接触する確率を0に制約する．
- $g(s,a) = -\,\mathbb{1}[\text{robot's body hits the floor}].$
#### 脚振り（Leg swing）制約
- 危険な脚振り動作を防ぐため，ヒップ関節の角速度が $2.0\,rad/s$ を超えないよう制約する．評価の便宜上，関節の角速度が $2.0\,rad/s$ を超えた場合はエピソードを打ち切る
- $g(s,a) = -\,\mathbb{1}[\text{Angular velocity exceeds 2.0 rad/s. }]..$

### エピソード値制約 (Episode value constraint)
- $\epsilon \ge \mathbb{E}_{\pi,\gamma}[\hat{g}(s,a)].$
- エピソード中に状態・行動から計算される値の割引重みつき期待値が $\epsilon$ 以下であることを制約する
- $g(s,a) = \epsilon - \hat{g}(s,a).$
- $\mathbb{E}_{\pi,\gamma}$ は割引分布下での期待値 


### QRSAC-Lagrangian
CaRを使用する際の強化学習問題を解決するために、QR-SACの拡張であるQRSAC-Lagrangianを提案
1. 方策パラメータ $\theta$ を初期化し，リプレイバッファ $D \leftarrow \varnothing$ を用意し，ラグランジュ乗数ベクトル $\lambda \leftarrow \mathbf{0}$ とする．
2. for each iteration i do
   1. 行動をサンプリング： $a_t \sim \pi_{\theta}(a_t\mid s_t)$
   2. 環境遷移： $s_{t+1} \sim p(s_{t+1}\mid s_t,a_t)$
   3. リプレイバッファへ格納：$D \leftarrow D \cup \{(s_t,a_t,r(s_t,a_t), g_1(s_t,a_t),\dots,g_M(s_t,a_t), s_{t+1})\}$
   4. バッファ $D$ と現在の $\lambda$ を用いて QRSAC により方策 $\pi_{\theta}$ を更新する（内部は QRSAC の更新）報酬は制約項を用いる？
   5. if $i \bmod d = 0$
      1. For each m do
      2. $\lambda_m \leftarrow \lambda_m - \operatorname{Adam}(\alpha_\lambda,\; \nabla_{\lambda_m} L(\pi,\lambda))$
      3. $\lambda_m \leftarrow \max(\lambda_m, 0)$ 
      4. EndFor
   6. EndIf
3. EndFor

### ロボットコントローラの設計
- この研究で用いるロボットは，目標関節角・位置を受け取って動作する PID コントローラにより制御される．  
- PID コントローラへの入力となる各脚の目標関節角・位置（次のステップのもの？）を出力するコントローラ $\pi_{\theta}$ を学習した
- 入力
  - 方策の入力は以下の特徴量を連結したベクトルである：
    - ヒップ関節角（6 次元）
    - 膝関節位置（6 次元）
    - ヒップ関節角速度（6 次元）
    - 膝関節速度（6 次元）
    - ロボットの角速度および線加速度（6 次元）
    - タスク開始からの経過タイムステップ（1 次元）
    - 方策出力の履歴（$12$ 次元 $\times H$ ステップ）
- 中間層（３✖️256）
- 出力層（24(目標関節角と目標関節位置の平均と標準偏差✖️6)）
- 履歴長はシミュレーションで $H=3$，実機実験で $H=5$ とした．

### 実験設定
- この実験では，任意の初期姿勢から直立姿勢へ遷移する方策を学習させ，10 種類の異なる初期姿勢で性能評価を行った（図1参照）．  
- シミュレーション環境は MuJoCo 上に構築し，方策は合計 1,000,000 stepで学習した．収束を早めるためにカリキュラム学習を行い，直立に近い初期姿勢から開始して徐々に直立姿勢からのずれを大きくしていった．  
- 学習時は並列シミュレーションは行わず，単一のシミュレータでデータ収集を行った．学習済みコントローラはすべて $10\,Hz$ で動作する．
- 実験で用いたハイパーパラメータは付録を参照

### 比較実験
シミュレーションでは提案手法を以下と比較評価した：
- 手動設計の報酬関数で学習した方策（タスクの学習しやすさを基に設計した5種類の報酬関数を表 II に示す）．
- 従来手法：提案する QRSAC-Lagrangian を SAC-Lagrangian，PPO-Lagrangian，および CaT などと比較．
- 訓練環境とは異なる地形での頑健性評価を行い，前節で設計した各制約の有効性を評価するアブレーションスタディも実施した．

### 実機実験
実機では，提案手法で学習した方策を実データでの微調整なしにそのまま適用し，実機上での性能を評価した．

### 結果
#### 図3
- 手動設計の報酬関数および提案手法で学習した方策を用いた際のロボットの最終姿勢を示す．括弧内には姿勢スコア $1/(||p_t|| + 1)$ を示している．
- 図から，提案手法（CaR）が目標姿勢への遷移に成功していることが確認できる．
- 手動設計の報酬関数で学習した方策はいずれも目標姿勢への遷移に失敗している．
- これらのうち Design 3 が最も高い最終姿勢スコアを示したが，それでも提案手法より低い結果となった．  
- 適切な報酬関数を設計すればこのタスクを達成することは不可能ではないが，本実験結果から，報酬設計が容易でないこと，および提案手法がそのような状況でも有効であることが確認できる．

#### 図４（a） 
- 提案手法と比較手法の学習曲線を示す．
- 5回の実行の平均
- 図より，提案手法である QRSAC-Lagrangian が，他の従来手法と比較してより高速かつ安定した収束を実現している．
- PPO-Lagrangian および CaT はタスクを学習できなかった．
- SAC-Lagrangian は最終的に同等の性能を示したが，収束速度は QRSAC-Lagrangian よりも遅い結果となった．  
- 同様の傾向は倒立振子タスクでも確認されており，結果は付録に示す．

#### 図４(b) 
- 提案手法により自動調整された重みパラメータの推移．
- pose parameterは左前 hip joint の結果（他も同じような）
- 学習初期では転倒制約の重みが素早く収束しており，これはロボットが早期に転倒を回避できるようになったためである．
- 姿勢制約・本体接触制約・脚スイング制約の重みは学習の進行に伴って徐々に増加しており，これらの制約を満たすことがより困難であったことを示している．
- 傾斜制約の重みは学習初期から中期にかけて増加し，終盤では減少した．これは，学習中期までにロボットが最終姿勢を床面と平行に保てるようになり，その後アルゴリズムが他の制約（姿勢・接触・スイング）を優先したためである．

#### 図５
- 学習済み方策を3種類の環境（平坦地形，粗地形，傾斜地形）で実行した結果．
- 平坦地形は学習環境であり，粗地形では最大 $0.05\,m$のランダムな高低差を持つ地形を使用．
- 傾斜地形は 10 度の傾きを持つ．
- 図から，提案手法で学習した方策が異なる地形環境でもタスクを達成しており，高いロバスト性を有することが確認できる．

#### 表3
- 設計した制約関数に対するアブレーションスタディの結果を示す．
- 表から，各制約がそれぞれ対応する性能指標の改善に寄与していることが確認できる．
- すべての制約を完全に満たすことは難しいものの，5 つすべての制約を組み合わせて学習した方策の平均性能は良好であった．  
- これらの結果を踏まえ，実機実験では 5 つすべての制約を用いて学習した方策を使用した．

#### 図6
- 実機上での学習済み方策の実行結果
- 図から，提案手法で学習した方策が実機でもタスクを成功裏に遂行できることが確認できる．
- シミュレーションで使用した 10 種類の初期姿勢に加えて，膝を伸ばしたより困難な 9 姿勢についても実験を行い，いずれの場合も直立姿勢への遷移に成功した．

### 結論
- この論文では，報酬関数の調整に伴う膨大な試行錯誤を軽減するために，Constraints as Rewards（CaR）という概念を導入した．  
- CaR は報酬関数を設計する代わりに，制約関数のみによってタスク目標を構成する手法である．  
- このアプローチでは，従来の報酬設計のようにタスク間の重みを手動で調整する必要がなく，学習過程で自動的に重みが更新される．  
- 制約関数の設計を直感的にするために，タスク目的を明確に解釈できる4種類の制約設計を提案した．  
- 本手法の学習問題を解くために，QRSAC-Lagrangian アルゴリズムを導入した．  
- 提案手法の有効性を確認するために，6脚伸縮型車輪ロボットTachyon 3の立ち上がり動作生成タスクに適用した．  
- このタスクは手動設計の報酬関数では学習が難しい課題であるが，提案手法を用いることでロボットが目標動作を効果的に学習できることを示した．  
- CaR は，幅広いロボットタスクに対して有効な枠組みであると考えられる．  
- CaR ではタスクを制約関数のみで表現する必要があるため，「できるだけ速く歩行する」など，純粋な最大化を目的とするタスクへの適用は難しい場合がある．  そのような場合には，報酬関数と制約関数を組み合わせることで，より効果的な目的関数設計が可能になると考えられる．  
- このようなタスクに対して有効な目的関数設計法を探求することが，将来の有望な研究課題である．  


